import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset, Subset, random_split
import torchvision
import torchvision.transforms as transforms
import time
import random
import copy
import numpy as np
import os
import sys
import json
from collections import defaultdict
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import matplotlib.pyplot as plt

# Set random seeds for reproducibility
def set_seed(seed=42):
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)

set_seed(42)

# =================== GLOBAL EARLY STOPPING FRAMEWORK ===================

class GlobalConvergenceTracker:
    """Research-grade global convergence tracking with early stopping"""
    def __init__(self, dataset_name, patience=20, accuracy_plateau=0.15, min_rounds=75):
        self.dataset_name = dataset_name
        self.patience = patience
        self.accuracy_plateau = accuracy_plateau
        self.min_rounds = min_rounds
        self.convergence_thresholds = {
            'MNIST': 95.0,
            'Fashion-MNIST': 85.0,
            'CIFAR-10': 75.0
        }
        self.min_accuracy = self.convergence_thresholds.get(dataset_name, 85.0)
        self.no_improvement_count = 0
        self.best_accuracy = 0.0
        self.convergence_round = None
        self.converged = False
        self.accuracy_history = []

    def update_accuracy(self, current_accuracy, round_num):
        """Update accuracy and check for convergence"""
        self.accuracy_history.append(current_accuracy)
        if round_num < self.min_rounds:
            return False
        if current_accuracy > self.best_accuracy + self.accuracy_plateau:
            self.best_accuracy = current_accuracy
            self.no_improvement_count = 0
        else:
            self.no_improvement_count += 1
        accuracy_threshold_met = current_accuracy >= self.min_accuracy
        plateau_reached = self.no_improvement_count >= self.patience
        if accuracy_threshold_met and plateau_reached and not self.converged:
            self.convergence_round = round_num
            self.converged = True
            return True
        return False

    def get_convergence_info(self):
        """Get comprehensive convergence information"""
        return {
            'converged': self.converged,
            'convergence_round': self.convergence_round,
            'best_accuracy': self.best_accuracy,
            'final_accuracy': self.accuracy_history[-1] if self.accuracy_history else 0.0,
            'min_accuracy_threshold': self.min_accuracy,
            'rounds_to_convergence': self.convergence_round if self.converged else None,
            'accuracy_at_convergence': self.best_accuracy if self.converged else None
        }

# =================== JSON SERIALIZATION ===================

def safe_serialize_value(value):
    """Recursively convert values to JSON-safe types"""
    if isinstance(value, (np.integer, np.int64, np.int32)):
        return int(value)
    elif isinstance(value, (np.floating, np.float64, np.float32)):
        return float(value)
    elif isinstance(value, np.ndarray):
        return value.tolist()
    elif isinstance(value, list):
        return [safe_serialize_value(item) for item in value]
    elif isinstance(value, dict):
        return {str(k): safe_serialize_value(v) for k, v in value.items()}
    elif isinstance(value, defaultdict):
        return {str(k): safe_serialize_value(v) for k, v in dict(value).items()}
    elif isinstance(value, set):
        return list(value)
    return value

def prepare_results_for_json(results):
    """Convert all results to JSON-serializable format"""
    return {str(key): safe_serialize_value(value) for key, value in results.items()}

# =================== STRAGGLER TRACKING ===================

class ResearchGradeStragglerTracker:
    """Research-grade straggler tracking"""
    def __init__(self, num_clients, client_speeds, straggler_threshold=1.5):
        self.num_clients = num_clients
        self.client_speeds = client_speeds
        self.straggler_threshold = straggler_threshold
        self.round_stragglers = []
        self.client_straggler_history = {cid: [] for cid in range(num_clients)}
        self.total_stragglers_per_round = []
        self.participation_metrics = {
            'slow_capable_clients': [i for i, speed in enumerate(client_speeds) if speed < 0.8],
            'normal_capable_clients': [i for i, speed in enumerate(client_speeds) if 0.8 <= speed <= 1.2],
            'fast_capable_clients': [i for i, speed in enumerate(client_speeds) if speed > 1.2],
        }
        self.accommodation_history = []

    def identify_round_stragglers_with_participation(self, client_times, round_num):
        """Enhanced straggler identification"""
        median_time = float(np.median(client_times))
        threshold = median_time * self.straggler_threshold
        round_stragglers = []
        for cid, time_taken in enumerate(client_times):
            is_straggler = time_taken > threshold
            round_stragglers.append(is_straggler)
            self.client_straggler_history[cid].append(is_straggler)
        straggler_count = int(sum(round_stragglers))
        self.round_stragglers.append(round_stragglers)
        self.total_stragglers_per_round.append(straggler_count)
        participation_analysis = self.analyze_participation_capability(client_times, threshold)
        self.accommodation_history.append(participation_analysis)
        return round_stragglers, straggler_count, participation_analysis

    def analyze_participation_capability(self, client_times, threshold):
        """Analyze algorithm's ability to accommodate stragglers"""
        slow_clients = self.participation_metrics['slow_capable_clients']
        normal_clients = self.participation_metrics['normal_capable_clients']
        fast_clients = self.participation_metrics['fast_capable_clients']
        accommodation_threshold = threshold * 1.2
        successful_slow = sum(1 for cid in slow_clients if client_times[cid] <= accommodation_threshold)
        successful_normal = sum(1 for cid in normal_clients if client_times[cid] <= threshold)
        successful_fast = sum(1 for cid in fast_clients if client_times[cid] <= threshold)
        total_slow = len(slow_clients)
        total_normal = len(normal_clients)
        total_fast = len(fast_clients)
        return {
            'total_clients': self.num_clients,
            'slow_capable_clients': total_slow,
            'normal_capable_clients': total_normal,
            'fast_capable_clients': total_fast,
            'successful_slow_participation': successful_slow,
            'successful_normal_participation': successful_normal,
            'successful_fast_participation': successful_fast,
            'slow_inclusion_rate': successful_slow / max(total_slow, 1),
            'normal_inclusion_rate': successful_normal / max(total_normal, 1),
            'fast_inclusion_rate': successful_fast / max(total_fast, 1),
            'stragglers_accommodated': successful_slow,
            'stragglers_excluded': total_slow - successful_slow,
            'accommodation_efficiency': successful_slow / max(total_slow, 1)
        }

    def get_comprehensive_straggler_statistics(self):
        """Get comprehensive straggler statistics"""
        if not self.total_stragglers_per_round:
            return {}
        avg_stragglers = float(np.mean(self.total_stragglers_per_round))
        straggler_rate = avg_stragglers / self.num_clients
        if self.accommodation_history:
            avg_accommodation = float(np.mean([h['accommodation_efficiency'] for h in self.accommodation_history]))
            avg_slow_inclusion = float(np.mean([h['slow_inclusion_rate'] for h in self.accommodation_history]))
        else:
            avg_accommodation = 0.0
            avg_slow_inclusion = 0.0
        persistent_stragglers = self.get_persistent_stragglers()
        return {
            'avg_stragglers_per_round': avg_stragglers,
            'max_stragglers_in_round': int(max(self.total_stragglers_per_round)),
            'min_stragglers_in_round': int(min(self.total_stragglers_per_round)),
            'straggler_rate': straggler_rate,
            'persistent_stragglers': persistent_stragglers,
            'straggler_variance': float(np.var(self.total_stragglers_per_round)),
            'avg_accommodation_efficiency': avg_accommodation,
            'avg_slow_inclusion_rate': avg_slow_inclusion,
            'total_slow_devices': len(self.participation_metrics['slow_capable_clients']),
            'accommodation_success_score': avg_accommodation * avg_slow_inclusion
        }

    def get_persistent_stragglers(self, persistence_threshold=0.7):
        """Identify clients that are frequently stragglers"""
        persistent = []
        for cid, history in self.client_straggler_history.items():
            if len(history) > 0:
                rate = sum(history) / len(history)
                if rate >= persistence_threshold:
                    persistent.append(int(cid))
        return persistent

# =================== EFFICIENCY TRACKING ===================

class ResearchGradeEfficiencyTracker:
    """Research-grade efficiency tracking for Fashion-MNIST"""
    def __init__(self, num_clients):
        self.num_clients = num_clients
        self.client_flops = defaultdict(list)
        self.client_memory_usage = defaultdict(list)
        self.client_energy_estimate = defaultdict(list)
        self.client_communication_costs = defaultdict(list)
        self.total_communication_per_round = []
        self.communication_breakdown = defaultdict(list)
        self.convergence_efficiency = {}
        self.parameter_efficiency = defaultdict(list)

    def start_client_monitoring(self, client_id):
        """Start monitoring computational resources"""
        return {
            'start_time': time.time(),
            'start_memory': torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        }

    def calculate_mlp_training_flops(self, model, batch_size, active_layers_count):
        """Calculate MLP training FLOPs"""
        total_flops = 0
        linear_layers = [module for module in model.model.modules() if isinstance(module, nn.Linear)]
        active_layers = linear_layers[:active_layers_count] if active_layers_count <= len(linear_layers) else linear_layers
        for layer in active_layers:
            forward_flops = batch_size * layer.in_features * layer.out_features
            backward_flops = 2 * forward_flops
            layer_flops = forward_flops + backward_flops
            total_flops += layer_flops
        return int(total_flops)

    def calculate_federated_communication_cost(self, strategy, full_model_state_dict, active_layers_count):
        """Calculate communication cost"""
        full_model_size = 0
        for param in full_model_state_dict.values():
            full_model_size += param.numel() * 4
        active_model_size = 0
        linear_layers = [name for name in full_model_state_dict.keys() if 'weight' in name or 'bias' in name]
        total_layers = len([name for name in linear_layers if 'weight' in name])
        if total_layers > 0:
            layers_to_include = min(active_layers_count, total_layers)
            layers_to_skip = max(0, total_layers - layers_to_include)
            layer_count = 0
            for name, param in full_model_state_dict.items():
                if 'weight' in name:
                    if layer_count >= layers_to_skip:
                        active_model_size += param.numel() * 4
                        bias_name = name.replace('weight', 'bias')
                        if bias_name in full_model_state_dict:
                            active_model_size += full_model_state_dict[bias_name].numel() * 4
                    layer_count += 1
        if strategy == "fedavg":
            downstream_bytes = full_model_size
            upstream_bytes = full_model_size
        elif strategy in ["fedpmt", "feddrop", "clamp"]:
            downstream_bytes = full_model_size
            upstream_bytes = active_model_size
        else:
            downstream_bytes = full_model_size
            upstream_bytes = full_model_size
        protocol_overhead = (downstream_bytes + upstream_bytes) * 0.05
        total_communication = downstream_bytes + upstream_bytes + protocol_overhead
        return {
            'downstream_bytes': int(downstream_bytes),
            'upstream_bytes': int(upstream_bytes),
            'protocol_overhead': int(protocol_overhead),
            'total_bytes': int(total_communication),
            'full_model_size': int(full_model_size),
            'active_model_size': int(active_model_size)
        }

    def end_client_monitoring(self, client_id, start_metrics, model_state_dict, layer_depth, strategy, batch_size=64):
        """End monitoring and calculate efficiency metrics"""
        end_time = time.time()
        end_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        training_time = end_time - start_metrics['start_time']
        memory_used = abs(end_memory - start_metrics['start_memory'])
        temp_model = FashionMNISTNet()
        flops = self.calculate_mlp_training_flops(temp_model, batch_size, layer_depth)
        comm_cost = self.calculate_federated_communication_cost(strategy, model_state_dict, layer_depth)
        estimated_energy = training_time * 45.0  # Higher for Fashion-MNIST
        self.client_flops[client_id].append(flops)
        self.client_memory_usage[client_id].append(int(memory_used))
        self.client_energy_estimate[client_id].append(estimated_energy)
        self.client_communication_costs[client_id].append(comm_cost['total_bytes'])
        return {
            'training_time': training_time,
            'flops': flops,
            'communication_cost': comm_cost,
            'energy': estimated_energy,
            'memory_used': int(memory_used)
        }

# =================== FASHION-MNIST MODEL ===================

class FashionMNISTNet(nn.Module):
    """Research-grade Fashion-MNIST neural network"""
    def __init__(self, input_size=784, hidden_sizes=[512, 256, 128, 64], num_classes=10, dropout_rate=0.4):
        super(FashionMNISTNet, self).__init__()
        layers = []
        prev_size = input_size
        for i, hidden_size in enumerate(hidden_sizes):
            layers.append(nn.Linear(prev_size, hidden_size))
            layers.append(nn.ReLU())
            if i < len(hidden_sizes) - 1:
                layers.append(nn.Dropout(dropout_rate))
            prev_size = hidden_size
        layers.append(nn.Linear(prev_size, num_classes))
        self.model = nn.Sequential(*layers)

    def forward(self, x):
        return self.model(x)

def apply_layer_mask(model, layers_to_update):
    """Apply mask to update only last 'layers_to_update' layers"""
    all_layers = [module for module in model.model.modules() if isinstance(module, nn.Linear)]
    total_linear_layers = len(all_layers)
    layers_to_freeze = max(0, total_linear_layers - layers_to_update)
    for i, layer in enumerate(all_layers):
        requires_grad = (i >= layers_to_freeze)
        if hasattr(layer, 'weight'):
            layer.weight.requires_grad = requires_grad
        if hasattr(layer, 'bias') and layer.bias is not None:
            layer.bias.requires_grad = requires_grad

# =================== TRAINING FUNCTIONS ===================

def train_locally_with_monitoring(model, train_loader, val_loader, layers_to_update, device,
                                 client_speed=1.0, max_epochs=20, lr=0.001, patience=7):
    """Enhanced local training with monitoring for Fashion-MNIST"""
    model = copy.deepcopy(model).to(device)
    apply_layer_mask(model, layers_to_update)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=1e-5)
    start_time = time.time()
    best_val_loss = float('inf')
    patience_counter = 0
    best_model_state = None
    train_losses = []
    val_losses = []
    val_accuracies = []

    model.train()
    for epoch in range(max_epochs):
        epoch_train_loss = 0
        num_batches = 0
        for batch_idx, (X, y) in enumerate(train_loader):
            X, y = X.to(device), y.to(device)
            optimizer.zero_grad()
            output = model(X)
            loss = criterion(output, y)
            loss.backward()
            optimizer.step()
            epoch_train_loss += loss.item()
            num_batches += 1
            if client_speed < 1.0:
                time.sleep(0.002 * (1.0 - client_speed))  # Slower for Fashion-MNIST

        avg_train_loss = epoch_train_loss / max(num_batches, 1)
        train_losses.append(avg_train_loss)

        model.eval()
        val_loss = 0
        correct = 0
        total = 0
        with torch.no_grad():
            for X_val, y_val in val_loader:
                X_val, y_val = X_val.to(device), y_val.to(device)
                outputs = model(X_val)
                val_loss += criterion(outputs, y_val).item()
                _, predicted = torch.max(outputs.data, 1)
                total += y_val.size(0)
                correct += (predicted == y_val).sum().item()

        avg_val_loss = val_loss / max(len(val_loader), 1)
        val_accuracy = 100 * correct / max(total, 1)
        val_losses.append(avg_val_loss)
        val_accuracies.append(val_accuracy)

        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            best_model_state = copy.deepcopy(model.state_dict())
        else:
            patience_counter += 1

        model.train()
        if patience_counter >= patience:
            break

    if best_model_state is not None:
        model.load_state_dict(best_model_state)

    elapsed_time = (time.time() - start_time) / client_speed
    final_train_loss = train_losses[-1] if train_losses else 0
    final_val_accuracy = val_accuracies[-1] if val_accuracies else 0
    epochs_trained = len(train_losses)

    return (model.state_dict(), elapsed_time, final_train_loss,
            final_val_accuracy, epochs_trained, train_losses, val_losses, val_accuracies)

# =================== CLAMP IMPLEMENTATION ===================

def adjust_layers_improved(elapsed_time, current_depth, client_id, T_low=12.0, T_high=35.0, max_layers=5):
    """Layer adjustment for Fashion-MNIST complexity"""
    client_T_low = T_low * (0.7 + client_id * 0.1)
    client_T_high = T_high * (0.8 + client_id * 0.2)
    if elapsed_time > client_T_high and current_depth > 1:
        return current_depth - 1
    elif elapsed_time < client_T_low and current_depth < max_layers:
        return current_depth + 1
    else:
        return current_depth

class CLAMPStabilityTracker:
    """CLAMP stability tracking"""
    def __init__(self, num_clients):
        self.adaptation_history = {cid: [] for cid in range(num_clients)}
        self.consecutive_suggestions = {cid: {'increase': 0, 'decrease': 0, 'stay': 0} for cid in range(num_clients)}
        self.performance_history = {cid: [] for cid in range(num_clients)}

    def should_adapt(self, client_id, current_depth, suggested_depth, client_performance=None, stability_threshold=3):
        """Enhanced adaptation decision"""
        if client_performance is not None:
            self.performance_history[client_id].append(client_performance)
        if suggested_depth == current_depth:
            self.consecutive_suggestions[client_id] = {'increase': 0, 'decrease': 0, 'stay': 0}
            return current_depth
        if suggested_depth > current_depth:
            self.consecutive_suggestions[client_id]['increase'] += 1
            self.consecutive_suggestions[client_id]['decrease'] = 0
            self.consecutive_suggestions[client_id]['stay'] = 0
            if self.consecutive_suggestions[client_id]['increase'] >= stability_threshold:
                self.consecutive_suggestions[client_id]['increase'] = 0
                return suggested_depth
        elif suggested_depth < current_depth:
            self.consecutive_suggestions[client_id]['decrease'] += 1
            self.consecutive_suggestions[client_id]['increase'] = 0
            self.consecutive_suggestions[client_id]['stay'] = 0
            if self.consecutive_suggestions[client_id]['decrease'] >= stability_threshold:
                self.consecutive_suggestions[client_id]['decrease'] = 0
                return suggested_depth
        return current_depth

# =================== EVALUATION ===================

def evaluate_model(model, test_loader, device):
    """Model evaluation"""
    model.eval()
    correct = 0
    total = 0
    total_loss = 0
    criterion = nn.CrossEntropyLoss()
    with torch.no_grad():
        for X, y in test_loader:
            X, y = X.to(device), y.to(device)
            outputs = model(X)
            loss = criterion(outputs, y)
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += y.size(0)
            correct += (predicted == y).sum().item()
    accuracy = 100 * correct / max(total, 1)
    avg_loss = total_loss / max(len(test_loader), 1)
    return float(accuracy), float(avg_loss)

def average_weights(weight_list, client_sizes=None):
    """Weighted average of model weights"""
    if client_sizes is None:
        client_sizes = [1] * len(weight_list)
    total_size = sum(client_sizes)
    avg_weights = copy.deepcopy(weight_list[0])
    for key in avg_weights:
        avg_weights[key] = torch.zeros_like(avg_weights[key])
    for i, weights in enumerate(weight_list):
        weight = client_sizes[i] / total_size
        for key in avg_weights:
            avg_weights[key] += weights[key] * weight
    return avg_weights

# =================== DATASET CREATION ===================

def create_federated_fashion_mnist(num_clients=50, batch_size=64, iid=True, val_split=0.2):
    """Create federated Fashion-MNIST dataset"""
    print("Downloading Fashion-MNIST dataset...")
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.2860,), (0.3530,)),  # Fashion-MNIST normalization
        transforms.Lambda(lambda x: x.view(-1))
    ])
    train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)
    test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    fashion_classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
                      'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
    total_samples = len(train_dataset)
    base_samples_per_client = total_samples // num_clients
    clients = []
    client_speeds = []

    print(f"Creating {num_clients} federated Fashion-MNIST clients...")
    for client_id in range(num_clients):
        if client_id < num_clients * 0.1:
            speed_factor = np.random.uniform(0.25, 0.55)  # Slower for Fashion-MNIST
        elif client_id < num_clients * 0.2:
            speed_factor = np.random.uniform(1.6, 2.3)
        else:
            speed_factor = np.random.uniform(0.75, 1.3)

        client_samples = base_samples_per_client
        if iid:
            indices = np.random.choice(range(total_samples), client_samples, replace=False)
        else:
            num_categories = np.random.randint(2, 5)
            target_classes = np.random.choice(range(10), size=num_categories, replace=False)
            indices = []
            targets = np.array(train_dataset.targets)
            for target_class in target_classes:
                class_indices = np.where(targets == target_class)[0]
                class_samples = min(len(class_indices), client_samples // len(target_classes))
                selected = np.random.choice(class_indices, class_samples, replace=False)
                indices.extend(selected)
            indices = indices[:client_samples]
            if client_id < 5:
                client_specialization = [fashion_classes[i] for i in target_classes]
                print(f"Client {client_id} specializes in: {client_specialization}")

        client_dataset = Subset(train_dataset, indices)
        train_size = int((1 - val_split) * len(client_dataset))
        val_size = len(client_dataset) - train_size
        train_subset, val_subset = random_split(client_dataset, [train_size, val_size])

        # Use same transform for validation
        val_subset.dataset.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.2860,), (0.3530,)),
            transforms.Lambda(lambda x: x.view(-1))
        ])

        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)
        clients.append((train_loader, val_loader, len(client_dataset)))
        client_speeds.append(speed_factor)

    print(f"Created {num_clients} Fashion-MNIST clients with {total_samples} total samples")
    return clients, test_loader, client_speeds

# =================== FEDERATED TRAINING WITHOUT LIMITS ===================

def research_grade_federated_training_fashion_mnist_until_convergence(clients, client_speeds, device,
                                                                     strategy="fedavg", max_layers=5, fixed_depth=3,
                                                                     test_loader=None, lr=0.001, max_local_epochs=20):
    """Federated training for Fashion-MNIST with NO ROUND LIMITS"""
    global_model = FashionMNISTNet().to(device)
    straggler_tracker = ResearchGradeStragglerTracker(len(clients), client_speeds)
    efficiency_tracker = ResearchGradeEfficiencyTracker(len(clients))
    convergence_tracker = GlobalConvergenceTracker('Fashion-MNIST', patience=20, accuracy_plateau=0.15, min_rounds=75)
    MAX_SAFETY_ROUNDS = 2000

    results = {
        'test_accuracies': [], 'test_losses': [], 'training_times': defaultdict(list),
        'client_depths': defaultdict(list), 'client_epochs_trained': defaultdict(list),
        'client_val_accuracies': defaultdict(list), 'round_times': [], 'losses': defaultdict(list),
        'convergence_info': {}, 'stragglers_per_round': [], 'client_straggler_history': defaultdict(list),
        'straggler_statistics': {}, 'participation_analysis': [], 'flops_per_round': [],
        'communication_costs_per_round': [], 'energy_consumption_per_round': [],
        'computational_efficiency': [], 'communication_efficiency': [],
        'client_communication_breakdown': defaultdict(list), 'rounds_to_convergence': None,
        'flops_until_convergence': 0, 'communication_until_convergence': 0,
        'energy_until_convergence': 0, 'accuracy_at_convergence': None,
        'early_stopped': False, 'safety_limit_reached': False
    }

    current_depths = {cid: 3 for cid in range(len(clients))}
    clamp_tracker = CLAMPStabilityTracker(len(clients)) if strategy == "clamp" else None

    print(f"\nRESEARCH-GRADE FASHION-MNIST FEDERATED LEARNING WITHOUT ROUND LIMITS")
    print(f"Strategy: {strategy.upper()}")
    print(f"Clients: {len(clients)}")
    print(f"Convergence Target: 85.0% accuracy | Patience: 20 rounds")
    print(f"Safety Limit: {MAX_SAFETY_ROUNDS} rounds")
    print(f"Will run until true convergence is achieved")
    print("-" * 80)

    r = 0
    while r < MAX_SAFETY_ROUNDS:
        round_start_time = time.time()
        print(f"\n--- Round {r+1} (No Limit Mode) ---")

        local_weights = []
        local_sizes = []
        round_training_times = []
        round_flops = []
        round_comm_costs = []
        round_energy = []

        for cid, (train_loader, val_loader, dataset_size) in enumerate(clients):
            if strategy == "fedavg":
                layer_depth = max_layers
            elif strategy == "fedpmt":
                layer_depth = fixed_depth
            elif strategy == "feddrop":
                layer_depth = random.randint(1, max_layers)
            elif strategy == "clamp":
                layer_depth = current_depths[cid]
            else:
                raise ValueError(f"Unknown strategy: {strategy}")

            results['client_depths'][cid].append(layer_depth)
            start_metrics = efficiency_tracker.start_client_monitoring(cid)

            training_results = train_locally_with_monitoring(
                global_model, train_loader, val_loader, layer_depth, device,
                client_speed=client_speeds[cid], max_epochs=max_local_epochs, lr=lr, patience=7
            )

            (local_state_dict, elapsed_time, final_train_loss,
             final_val_accuracy, epochs_trained, train_losses, val_losses, val_accuracies) = training_results

            efficiency_metrics = efficiency_tracker.end_client_monitoring(
                cid, start_metrics, local_state_dict, layer_depth, strategy
            )

            results['training_times'][cid].append(elapsed_time)
            results['losses'][cid].append(final_train_loss)
            results['client_epochs_trained'][cid].append(epochs_trained)
            results['client_val_accuracies'][cid].append(final_val_accuracy)
            results['client_communication_breakdown'][cid].append(efficiency_metrics['communication_cost'])

            round_training_times.append(elapsed_time)
            round_flops.append(efficiency_metrics['flops'])
            round_comm_costs.append(efficiency_metrics['communication_cost']['total_bytes'])
            round_energy.append(efficiency_metrics['energy'])

            if cid < 3:
                comm = efficiency_metrics['communication_cost']
                print(f"Client {cid:2d}: depth={layer_depth}, time={elapsed_time:5.1f}s, "
                      f"FLOPs={efficiency_metrics['flops']/1e6:.1f}M, "
                      f"Comm={comm['total_bytes']/1024:.1f}KB, Energy={efficiency_metrics['energy']:.1f}J")

            if strategy == "clamp":
                suggested_depth = adjust_layers_improved(elapsed_time, layer_depth, cid,
                                                       T_low=12.0, T_high=35.0, max_layers=max_layers)
                new_depth = clamp_tracker.should_adapt(cid, layer_depth, suggested_depth,
                                                    client_performance=final_val_accuracy, stability_threshold=3)
                current_depths[cid] = new_depth

            local_weights.append(local_state_dict)
            local_sizes.append(dataset_size)

        round_stragglers, straggler_count, participation_analysis = straggler_tracker.identify_round_stragglers_with_participation(
            round_training_times, r
        )

        results['stragglers_per_round'].append(straggler_count)
        results['participation_analysis'].append(participation_analysis)
        results['flops_per_round'].append(int(np.sum(round_flops)))
        results['communication_costs_per_round'].append(int(np.sum(round_comm_costs)))
        results['energy_consumption_per_round'].append(float(np.sum(round_energy)))

        if results['test_accuracies']:
            prev_acc = results['test_accuracies'][-1]
        else:
            prev_acc = 0

        current_flops = np.sum(round_flops)
        current_comm = np.sum(round_comm_costs)
        comp_efficiency = prev_acc / max(current_flops / 1e9, 1e-10)
        comm_efficiency = prev_acc / max(current_comm / 1e6, 1e-10)
        results['computational_efficiency'].append(comp_efficiency)
        results['communication_efficiency'].append(comm_efficiency)

        for cid, is_straggler in enumerate(round_stragglers):
            results['client_straggler_history'][cid].append(is_straggler)

        straggler_ids = [cid for cid, is_straggler in enumerate(round_stragglers) if is_straggler]
        median_time = float(np.median(round_training_times))
        avg_flops = float(np.mean(round_flops)) / 1e6
        total_comm = float(np.sum(round_comm_costs)) / 1024
        total_energy = float(np.sum(round_energy))

        print(f"\nRound {r+1} Fashion-MNIST Analysis:")
        print(f"   Median Training Time: {median_time:.1f}s")
        print(f"   Stragglers: {straggler_count}/{len(clients)} ({straggler_count/len(clients)*100:.1f}%)")
        print(f"   Slow Device Inclusion: {participation_analysis['slow_inclusion_rate']*100:.1f}%")
        print(f"   Total MLP FLOPs: {avg_flops*len(clients):.1f}M")
        print(f"   Total Communication: {total_comm:.1f}KB")

        global_weights = average_weights(local_weights, local_sizes)
        global_model.load_state_dict(global_weights)

        if test_loader:
            test_acc, test_loss = evaluate_model(global_model, test_loader, device)
            results['test_accuracies'].append(test_acc)
            results['test_losses'].append(test_loss)

            print(f"   Global Test Accuracy: {test_acc:.2f}%")

            converged = convergence_tracker.update_accuracy(test_acc, r + 1)
            if converged:
                print(f"   *** GLOBAL CONVERGENCE DETECTED at Round {r+1} ***")
                print(f"   *** Accuracy: {test_acc:.2f}% >= Threshold: {convergence_tracker.min_accuracy}% ***")
                print(f"   *** TRUE CONVERGENCE ACHIEVED - STOPPING ***")

                results['rounds_to_convergence'] = r + 1
                results['flops_until_convergence'] = int(np.sum(results['flops_per_round']))
                results['communication_until_convergence'] = int(np.sum(results['communication_costs_per_round']))
                results['energy_until_convergence'] = float(np.sum(results['energy_consumption_per_round']))
                results['accuracy_at_convergence'] = test_acc
                results['early_stopped'] = True
                break

        round_time = time.time() - round_start_time
        results['round_times'].append(round_time)

        if (r + 1) % 50 == 0:
            print(f"\nProgress: {r+1} rounds completed (no artificial limit)")
            if results['test_accuracies']:
                current_acc = results['test_accuracies'][-1]
                best_acc = convergence_tracker.best_accuracy
                print(f"   Current Fashion-MNIST Accuracy: {current_acc:.2f}%")
                print(f"   Best Accuracy So Far: {best_acc:.2f}%")
                print(f"   Target for Convergence: {convergence_tracker.min_accuracy}%")

        r += 1

    if r >= MAX_SAFETY_ROUNDS:
        print(f"\n*** SAFETY LIMIT REACHED at Round {r} ***")
        results['rounds_to_convergence'] = r
        results['flops_until_convergence'] = int(np.sum(results['flops_per_round']))
        results['communication_until_convergence'] = int(np.sum(results['communication_costs_per_round']))
        results['energy_until_convergence'] = float(np.sum(results['energy_consumption_per_round']))
        results['accuracy_at_convergence'] = results['test_accuracies'][-1] if results['test_accuracies'] else 0.0
        results['early_stopped'] = False
        results['safety_limit_reached'] = True

    convergence_info = convergence_tracker.get_convergence_info()
    results['convergence_info'] = convergence_info
    results['straggler_statistics'] = straggler_tracker.get_comprehensive_straggler_statistics()

    return global_model, results, straggler_tracker, efficiency_tracker

# =================== ENHANCED ANALYSIS FUNCTIONS ===================

def analyze_comprehensive_research_performance_with_inclusion_analysis(all_results, strategies, straggler_trackers, efficiency_trackers, dataset_name):
    """Enhanced analysis with detailed straggler inclusion metrics"""
    print(f"\n{'='*80}")
    print(f"RESEARCH-GRADE {dataset_name.upper()} PERFORMANCE ANALYSIS (NO ROUND LIMITS)")
    print("True Convergence Analysis with Straggler Inclusion Metrics")
    print(f"{'='*80}")

    analysis_summary = {}

    for i, strategy in enumerate(strategies):
        results = all_results[i]
        straggler_stats = results.get('straggler_statistics', {})
        straggler_tracker = straggler_trackers[i]

        if not results.get('test_accuracies'):
            continue

        final_accuracy = float(results['test_accuracies'][-1])
        rounds_to_convergence = results.get('rounds_to_convergence', len(results['test_accuracies']))
        early_stopped = results.get('early_stopped', False)
        safety_limit_reached = results.get('safety_limit_reached', False)

        flops_until_convergence = results.get('flops_until_convergence', 0)
        comm_until_convergence = results.get('communication_until_convergence', 0)
        energy_until_convergence = results.get('energy_until_convergence', 0.0)
        total_time = float(np.sum(results.get('round_times', [])[:rounds_to_convergence]))

        # Calculate straggler inclusion metrics
        total_stragglers = int(np.sum(results.get('stragglers_per_round', [])[:rounds_to_convergence]))
        avg_stragglers_per_round = straggler_stats.get('avg_stragglers_per_round', 0)
        straggler_rate = straggler_stats.get('straggler_rate', 0)
        slow_inclusion_rate = straggler_stats.get('avg_slow_inclusion_rate', 0)
        accommodation_efficiency = straggler_stats.get('avg_accommodation_efficiency', 0)
        total_slow_devices = straggler_stats.get('total_slow_devices', 0)

        # Efficiency metrics
        convergence_computational_efficiency = final_accuracy / max(flops_until_convergence / 1e9, 1e-10)
        convergence_communication_efficiency = final_accuracy / max(comm_until_convergence / 1e6, 1e-10)
        convergence_time_efficiency = final_accuracy / max(total_time, 1e-10)

        if early_stopped:
            convergence_status = "CONVERGED EARLY"
        elif safety_limit_reached:
            convergence_status = "NO CONVERGENCE"
        else:
            convergence_status = "NO CONVERGENCE"

        analysis_summary[strategy] = {
            'final_accuracy': final_accuracy,
            'rounds_to_convergence': rounds_to_convergence,
            'convergence_status': convergence_status,
            'convergence_computational_efficiency': convergence_computational_efficiency,
            'convergence_communication_efficiency': convergence_communication_efficiency,
            'convergence_time_efficiency': convergence_time_efficiency,
            'slow_inclusion_rate': slow_inclusion_rate,
            'accommodation_efficiency': accommodation_efficiency,
            'total_stragglers': total_stragglers,
            'straggler_rate': straggler_rate
        }

        print(f"\n{strategy.upper()} - {dataset_name} Convergence Analysis:")
        print(f"   Final Accuracy: {final_accuracy:.2f}%")
        print(f"   Convergence Status: {convergence_status}")
        print(f"   Rounds to Convergence: {rounds_to_convergence}")
        print(f"   Time to Convergence: {total_time:.1f}s ({total_time/60:.1f} min)")
        print(f"   FLOPs until Convergence: {flops_until_convergence/1e9:.2f} GFLOP")
        print(f"   Communication until Convergence: {comm_until_convergence/1e6:.2f} MB")
        print(f"   Energy until Convergence: {energy_until_convergence:.1f} J")
        print(f"   Stragglers until Convergence: {total_stragglers}")
        print(f"   Average Straggler Rate: {straggler_rate*100:.1f}%")
        print(f"   Total Slow Devices: {total_slow_devices}")
        print(f"   Slow Device Inclusion: {slow_inclusion_rate*100:.1f}%")
        print(f"   Accommodation Efficiency: {accommodation_efficiency*100:.1f}%")
        print(f"   CONVERGENCE Computational Efficiency: {convergence_computational_efficiency:.4f} acc/GFLOP")
        print(f"   CONVERGENCE Communication Efficiency: {convergence_communication_efficiency:.4f} acc/MB")
        print(f"   CONVERGENCE Time Efficiency: {convergence_time_efficiency:.4f} acc/s")

    return analysis_summary

def plot_enhanced_convergence_analysis_with_cdf(all_results, strategies, dataset_name):
    """Enhanced plotting with CDF analysis and straggler inclusion metrics"""
    fig = plt.figure(figsize=(20, 15))
    
    # Create a 3x3 grid for comprehensive analysis
    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
    
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
    markers = ['o', 's', '^', 'D']
    
    # Set convergence thresholds by dataset
    convergence_thresholds = {
        'MNIST': 95.0,
        'Fashion-MNIST': 85.0,
        'CIFAR-10': 75.0
    }
    threshold = convergence_thresholds.get(dataset_name, 85.0)
    
    # Set safety limits by dataset
    safety_limits = {
        'MNIST': 2000,
        'Fashion-MNIST': 2000,
        'CIFAR-10': 3000
    }
    default_safety = safety_limits.get(dataset_name, 2000)
    
    fig.suptitle(f'{dataset_name} - Enhanced Convergence Analysis with Straggler Inclusion', 
                 fontsize=16, fontweight='bold')

    # 1. Accuracy Evolution (top-left)
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.set_title('Accuracy Evolution', fontsize=12, fontweight='bold')
    for i, (strategy, results) in enumerate(zip(strategies, all_results)):
        if results['test_accuracies']:
            rounds = range(1, len(results['test_accuracies'])+1)
            ax1.plot(rounds, results['test_accuracies'], label=f'{strategy.upper()}', 
                    color=colors[i], marker=markers[i], markersize=3, markevery=max(1, len(rounds)//20))
            if results.get('early_stopped', False):
                conv_round = results.get('rounds_to_convergence', len(results['test_accuracies']))
                conv_acc = results['test_accuracies'][conv_round-1] if conv_round <= len(results['test_accuracies']) else results['test_accuracies'][-1]
                ax1.scatter(conv_round, conv_acc, color=colors[i], s=100, marker='*', edgecolor='black')

    ax1.axhline(y=threshold, color='red', linestyle='--', alpha=0.7, label=f'Convergence Threshold ({threshold}%)')
    ax1.set_xlabel('Round')
    ax1.set_ylabel('Test Accuracy (%)')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 2. Rounds to Convergence (top-center)
    ax2 = fig.add_subplot(gs[0, 1])
    ax2.set_title('Rounds to Convergence', fontsize=12, fontweight='bold')
    conv_rounds = []
    strategy_names = []
    for i, (strategy, results) in enumerate(zip(strategies, all_results)):
        rounds = results.get('rounds_to_convergence', default_safety)
        conv_rounds.append(rounds)
        strategy_names.append(strategy.upper())

    bars = ax2.bar(strategy_names, conv_rounds, color=colors[:len(strategies)], alpha=0.8)
    ax2.set_ylabel('Rounds to Convergence')
    ax2.tick_params(axis='x', rotation=45)

    for bar, val, result in zip(bars, conv_rounds, all_results):
        label = f'{val}' if result.get('early_stopped', False) else f'{val}*'
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(conv_rounds)*0.01,
                label, ha='center', va='bottom', fontweight='bold')

    # 3. Straggler Inclusion Rate (top-right)
    ax3 = fig.add_subplot(gs[0, 2])
    ax3.set_title('Slow Device Inclusion Rate', fontsize=12, fontweight='bold')
    inclusion_rates = []
    for strategy, results in zip(strategies, all_results):
        straggler_stats = results.get('straggler_statistics', {})
        inclusion_rate = straggler_stats.get('avg_slow_inclusion_rate', 0) * 100
        inclusion_rates.append(inclusion_rate)

    bars = ax3.bar(strategy_names, inclusion_rates, color=colors[:len(strategies)], alpha=0.8)
    ax3.set_ylabel('Inclusion Rate (%)')
    ax3.tick_params(axis='x', rotation=45)
    ax3.set_ylim(0, 100)

    for bar, val in zip(bars, inclusion_rates):
        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,
                f'{val:.1f}%', ha='center', va='bottom', fontweight='bold')

    # 4. Communication Cost CDF (middle-left)
    ax4 = fig.add_subplot(gs[1, 0])
    ax4.set_title('Communication Cost CDF', fontsize=12, fontweight='bold')
    for i, (strategy, results) in enumerate(zip(strategies, all_results)):
        comm_costs = results.get('communication_costs_per_round', [])
        if comm_costs:
            comm_costs_kb = [cost/1024 for cost in comm_costs]  # Convert to KB
            sorted_costs = np.sort(comm_costs_kb)
            p = np.arange(1, len(sorted_costs) + 1) / len(sorted_costs)
            ax4.plot(sorted_costs, p, label=f'{strategy.upper()}', color=colors[i], marker=markers[i], markersize=4, markevery=max(1, len(sorted_costs)//20))

    ax4.set_xlabel('Communication Cost (KB)')
    ax4.set_ylabel('CDF')
    ax4.legend()
    ax4.grid(True, alpha=0.3)

    # 5. Training Time CDF (middle-center)
    ax5 = fig.add_subplot(gs[1, 1])
    ax5.set_title('Round Training Time CDF', fontsize=12, fontweight='bold')
    for i, (strategy, results) in enumerate(zip(strategies, all_results)):
        round_times = results.get('round_times', [])
        if round_times:
            sorted_times = np.sort(round_times)
            p = np.arange(1, len(sorted_times) + 1) / len(sorted_times)
            ax5.plot(sorted_times, p, label=f'{strategy.upper()}', color=colors[i], marker=markers[i], markersize=4, markevery=max(1, len(sorted_times)//20))

    ax5.set_xlabel('Round Time (seconds)')
    ax5.set_ylabel('CDF')
    ax5.legend()
    ax5.grid(True, alpha=0.3)

    # 6. FLOPs CDF (middle-right)
    ax6 = fig.add_subplot(gs[1, 2])
    ax6.set_title('Computational Cost CDF', fontsize=12, fontweight='bold')
    for i, (strategy, results) in enumerate(zip(strategies, all_results)):
        flops_per_round = results.get('flops_per_round', [])
        if flops_per_round:
            flops_gflop = [flops/1e9 for flops in flops_per_round]  # Convert to GFLOP
            sorted_flops = np.sort(flops_gflop)
            p = np.arange(1, len(sorted_flops) + 1) / len(sorted_flops)
            ax6.plot(sorted_flops, p, label=f'{strategy.upper()}', color=colors[i], marker=markers[i], markersize=4, markevery=max(1, len(sorted_flops)//20))

    ax6.set_xlabel('FLOPs (GFLOP)')
    ax6.set_ylabel('CDF')
    ax6.legend()
    ax6.grid(True, alpha=0.3)

    # 7. Overall Efficiency Comparison (bottom-left)
    ax7 = fig.add_subplot(gs[2, 0])
    ax7.set_title('Computational Efficiency', fontsize=12, fontweight='bold')
    comp_eff = []
    for strategy, results in zip(strategies, all_results):
        final_acc = results['test_accuracies'][-1] if results['test_accuracies'] else 0
        total_flops = results.get('flops_until_convergence', 1)
        eff = final_acc / max(total_flops / 1e9, 1e-10)
        comp_eff.append(eff)

    bars = ax7.bar(strategy_names, comp_eff, color=colors[:len(strategies)], alpha=0.8)
    ax7.set_ylabel('Accuracy / GFLOP')
    ax7.tick_params(axis='x', rotation=45)

    # 8. Communication Efficiency (bottom-center)
    ax8 = fig.add_subplot(gs[2, 1])
    ax8.set_title('Communication Efficiency', fontsize=12, fontweight='bold')
    comm_eff = []
    for strategy, results in zip(strategies, all_results):
        final_acc = results['test_accuracies'][-1] if results['test_accuracies'] else 0
        total_comm = results.get('communication_until_convergence', 1)
        eff = final_acc / max(total_comm / 1e6, 1e-10)
        comm_eff.append(eff)

    bars = ax8.bar(strategy_names, comm_eff, color=colors[:len(strategies)], alpha=0.8)
    ax8.set_ylabel('Accuracy / MB')
    ax8.tick_params(axis='x', rotation=45)

    # 9. Straggler Management Efficiency (bottom-right)
    ax9 = fig.add_subplot(gs[2, 2])
    ax9.set_title('Straggler Management Score', fontsize=12, fontweight='bold')
    straggler_scores = []
    for strategy, results in zip(strategies, all_results):
        straggler_stats = results.get('straggler_statistics', {})
        inclusion_rate = straggler_stats.get('avg_slow_inclusion_rate', 0)
        accommodation_eff = straggler_stats.get('avg_accommodation_efficiency', 0)
        # Combined score: inclusion rate * accommodation efficiency
        score = inclusion_rate * accommodation_eff * 100
        straggler_scores.append(score)

    bars = ax9.bar(strategy_names, straggler_scores, color=colors[:len(strategies)], alpha=0.8)
    ax9.set_ylabel('Management Score')
    ax9.tick_params(axis='x', rotation=45)

    for bar, val in zip(bars, straggler_scores):
        ax9.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(straggler_scores)*0.01,
                f'{val:.1f}', ha='center', va='bottom', fontweight='bold')

    plt.tight_layout()
    plot_filename = f"{dataset_name.lower()}_enhanced_convergence_analysis"
    plt.show(plot_filename)

# Modified main analysis function to use enhanced features
def run_enhanced_analysis_with_inclusion_metrics(all_results, straggler_trackers, efficiency_trackers, strategies, dataset_name):
    """Run the enhanced analysis with straggler inclusion metrics and CDF plots"""
    
    # Generate enhanced console analysis
    analysis_summary = analyze_comprehensive_research_performance_with_inclusion_analysis(
        all_results, strategies, straggler_trackers, efficiency_trackers, dataset_name
    )
    
    # Generate enhanced plots with CDF analysis
    plot_enhanced_convergence_analysis_with_cdf(all_results, strategies, dataset_name)
    
    return analysis_summary

def run_research_grade_fashion_mnist_experiment_no_limits(iid=True, num_clients=50, max_local_epochs=20):
    """Run Fashion-MNIST experiment with no round limits"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    data_type = "IID" if iid else "Non-IID"
    print(f"Creating federated Fashion-MNIST dataset ({data_type})...")
    clients, test_loader, client_speeds = create_federated_fashion_mnist(
        num_clients=num_clients, batch_size=64, iid=iid, val_split=0.2
    )

    strategies = ["fedavg", "fedpmt", "feddrop", "clamp"]
    all_results = []
    all_straggler_trackers = []
    all_efficiency_trackers = []

    print(f"\n{'='*80}")
    print(f"RESEARCH-GRADE FASHION-MNIST MLP FEDERATED LEARNING - NO ROUND LIMITS")
    print(f"Scientific Rigor: Run Until True Mathematical Convergence")
    print(f"Convergence Target: 85.0% accuracy | Safety Limit: 2000 rounds")
    print(f"Dataset: Fashion-MNIST (10 fashion classes)")
    print(f"Architecture: 4-layer MLP")
    print(f"{'='*80}")

    for strategy in strategies:
        print(f"\n{'='*80}")
        print(f"Running {strategy.upper()} - Fashion-MNIST MLP Until True Convergence")
        print(f"{'='*80}")

        set_seed(42)
        start_time = time.time()

        global_model, results, straggler_tracker, efficiency_tracker = research_grade_federated_training_fashion_mnist_until_convergence(
            clients=clients, client_speeds=client_speeds, device=device, strategy=strategy,
            max_layers=5, fixed_depth=3, test_loader=test_loader, lr=0.001, max_local_epochs=max_local_epochs
        )

        experiment_time = time.time() - start_time
        all_results.append(results)
        all_straggler_trackers.append(straggler_tracker)
        all_efficiency_trackers.append(efficiency_tracker)

        if results['test_accuracies']:
            final_acc = results['test_accuracies'][-1]
            rounds_to_conv = results.get('rounds_to_convergence', 2000)
            early_stopped = results.get('early_stopped', False)

            conv_status = "TRUE CONVERGENCE ACHIEVED" if early_stopped else "NO CONVERGENCE"
            print(f"\n{strategy.upper()} TRUE CONVERGENCE RESULTS:")
            print(f"   Final Accuracy: {final_acc:.2f}%")
            print(f"   Convergence Status: {conv_status}")
            print(f"   Rounds to Convergence: {rounds_to_conv}")
            print(f"   Total Experiment Time: {experiment_time/60:.1f} minutes")

    print(f"\n{'='*80}")
    print("GENERATING FASHION-MNIST TRUE CONVERGENCE ANALYSIS (NO ROUND LIMITS)")
    print(f"{'='*80}")

    analysis_summary = run_enhanced_analysis_with_inclusion_metrics(
        all_results, all_straggler_trackers, all_efficiency_trackers, strategies, "Fashion-MNIST"
    )

    return all_results, all_straggler_trackers, all_efficiency_trackers, analysis_summary

# =================== SETUP ===================

def setup_research_environment():
    """Setup research environment"""
    plt.ioff()
    if torch.cuda.is_available():
        print(f"CUDA available: {torch.cuda.get_device_name()}")
    else:
        print("CUDA not available, using CPU")
    os.makedirs('./data', exist_ok=True)
    os.makedirs('./results', exist_ok=True)
    os.makedirs('./plots', exist_ok=True)

def save_plot_for_publication():
    """Replace plt.show() with publication-quality saving"""
    original_show = plt.show
    def publication_show(filename_prefix="plot"):
        timestamp = int(time.time())
        filename = f"./plots/{filename_prefix}_{timestamp}.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white')
        print(f"Publication-quality plot saved: {filename}")
        plt.close()
    plt.show = publication_show

# =================== MAIN EXECUTION ===================

if __name__ == "__main__":
    setup_research_environment()
    save_plot_for_publication()

    NUM_CLIENTS = 50
    MAX_LOCAL_EPOCHS = 20  # Higher for Fashion-MNIST

    print("Starting Research-Grade Fashion-MNIST Federated Learning with NO ROUND LIMITS")
    print("Will run until true mathematical convergence is achieved")
    print(f"Convergence Target: 85.0% accuracy")
    print(f"Safety Limit: 2000 rounds (prevents infinite loops)")
    print(f"Clients: {NUM_CLIENTS} | Strategies: FedAvg, FedPMT, FedDrop, CLAMP")
    print("\n*** WARNING: This may take MUCH longer than fixed-round experiments ***")
    print("*** Fashion-MNIST is more challenging than MNIST ***")

    experiment_results = run_research_grade_fashion_mnist_experiment_no_limits(
        iid=True, num_clients=NUM_CLIENTS, max_local_epochs=MAX_LOCAL_EPOCHS
    )

    if experiment_results:
        all_results, straggler_trackers, efficiency_trackers, analysis_summary = experiment_results
        print("\nResearch-Grade Fashion-MNIST No-Limits Experiment Complete!")

        try:
            serializable_results = []
            for result in all_results:
                serializable_result = prepare_results_for_json(result)
                serializable_results.append(serializable_result)

            research_data = {
                'experiment_results': serializable_results,
                'analysis_summary': prepare_results_for_json(analysis_summary),
                'experiment_config': {
                    'max_rounds': 'No Limit (Safety: 2000)',
                    'num_clients': NUM_CLIENTS,
                    'max_local_epochs': MAX_LOCAL_EPOCHS,
                    'convergence_threshold': 85.0,
                    'round_limits_removed': True,
                    'strategies': ["fedavg", "fedpmt", "feddrop", "clamp"],
                    'dataset': 'Fashion-MNIST', 'architecture': 'MLP'
                },
                'metadata': {
                    'timestamp': time.time(),
                    'convergence_framework': 'True Mathematical Convergence (No Artificial Limits)',
                    'scientific_rigor_improvements': [
                        'Removed artificial round limits',
                        'True convergence detection only',
                        'Safety mechanisms to prevent infinite loops',
                        'Unbiased algorithm capability assessment',
                        'Fashion-MNIST specific parameters',
                        'Enhanced straggler inclusion analysis',
                        'CDF plots for comprehensive performance evaluation'
                    ]
                }
            }

            with open('./results/fashion_mnist_no_limits_true_convergence_results.json', 'w') as f:
                json.dump(research_data, f, indent=2)

            print("Results saved to ./results/fashion_mnist_no_limits_true_convergence_results.json")
            print("Fashion-MNIST True Convergence experiment completed successfully!")

        except Exception as e:
            print(f"Warning: Could not save JSON results due to: {e}")
    else:
        print("Fashion-MNIST no-limits experiment failed!")
        sys.exit(1)
